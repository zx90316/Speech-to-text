# AI 語音轉文字系統 - 開發文件

> **文件版本:** 1.0
> **目標 Python 版本:** 3.11
> **最後更新:** 2025-08-14

---

## 1. 系統概述

本專案旨在開發一個功能完善的語音轉文字 Web 應用程式。使用者可以上傳音訊檔案，並選擇使用 **Google Cloud Vertex AI** 或**遠端自部署的開源 LLM** 進行語音辨識。系統需提供即時的處理進度，並在完成後讓使用者下載多種格式的文字稿。

### 1.1. 核心功能需求

* **前端介面**：提供一個直觀的網頁，用於上傳音訊檔案（如 `.mp3`, `.wav`, `.m4a`）。
* **模型選擇**：使用者可在前端自由選擇使用 `Vertex AI` 或 `遠端本地模型` 進行處理。
* **時間軸選取**：使用者可選取音訊的特定區段進行轉錄，而非必須處理整個檔案。
* **即時進度反饋**：
    * 顯示一個進度條，告知使用者目前的處理進度百分比。
    * 即時顯示已經辨識完成的文字片段。
* **多格式結果下載**：任務完成後，使用者可下載以下格式的文字稿：
    * **純文字 (.txt)**：僅包含完整的辨識文字。
    * **帶時間戳記 (.txt)**：每段文字前標註其在音訊中的開始與結束時間。
    * **字幕檔 (.srt)**：標準的影片字幕格式，方便後續應用。
* **後端處理**：後端需能處理長時間、大檔案的音訊，並透過非同步任務佇列管理轉錄工作。

### 1.2. 技術棧

* **後端框架**: **FastAPI** - 高效能、易於開發的 Python Web 框架。
* **前端框架**: **React** 或 **Vue.js** - 現代化的前端框架，用於打造互動式使用者介面。
* **任務佇列**: **Celery** & **Redis** - 用於處理耗時的背景轉錄任務，避免請求超時。
* **即時通訊**: **WebSocket** - 用於將後端處理進度即時推播給前端。
* **遠端模型部署**: 使用 **FastAPI** 或 **Gradio** 將開源模型包裝成 API 服務。

---

## 2. 系統架構設計

本系統採用前後端分離架構，並設計了兩種語音辨識路徑。

```mermaid
graph TD
    A[使用者] -->|1. 上傳檔案 & 選擇模型| B(前端介面 - React/Vue);
    B -->|2. 發送請求 (檔案, 模型選項)| C{後端主伺服器 (FastAPI)};
    C -->|3. 建立背景任務| D[任務佇列 (Celery + Redis)];
    C -->|4. 建立 WebSocket 連線| A;

    subgraph "非同步轉錄工作"
        E[Worker 背景工作者] -->|5. 從佇列取任務| D;
        E -->|6. 判斷模型選擇| F{模型路由};
        F -- "選擇 Vertex AI" --> G[Google Vertex AI API];
        F -- "選擇遠端模型" --> H[遠端模型伺服器 (RTX 5060 Ti)];
    end

    subgraph "結果回傳"
        G -->|7a. 回傳文字片段| E;
        H -->|7b. 回傳文字片段| E;
        E -->|8. 將進度/結果透過 WebSocket 推播| C;
        C -->|9. 顯示進度/文字| A;
    end

    A -->|10. 任務完成後請求下載| C;
    C -->|11. 從資料庫/快取生成檔案| A;
```

### 流程說明：

1.  **使用者操作**：在前端上傳音訊檔案，並選擇要使用的辨識模型（Vertex AI 或遠端模型）。
2.  **API 請求**：前端將檔案和選項透過 HTTP POST 請求傳送至後端 FastAPI 主伺服器。
3.  **任務建立**：後端收到請求後，不直接處理，而是立即建立一個 Celery 背景任務，並將任務 ID 回傳給前端。
4.  **即時通訊**：前端利用任務 ID 與後端建立 WebSocket 連線，用於接收即時進度。
5.  **背景處理**：Celery Worker 從 Redis 佇列中取得任務，開始處理音訊。
6.  **模型路由**：Worker 根據任務參數，決定呼叫 Vertex AI 或是遠端的自部署模型 API。
7.  **語音辨識**：
    * **音訊分塊 (Chunking)**：為處理長音訊，Worker 會先將音訊切成數個小塊（例如 1-5 分鐘）。
    * **迭代處理**：Worker 逐一將音訊塊傳送給目標 API 進行辨識。
8.  **進度推播**：每完成一個音訊塊的辨識，Worker 就會計算當前總進度，並將進度百分比和已辨識的文字片段透過 WebSocket 推播給前端。
9.  **前端顯示**：前端接收到 WebSocket 訊息後，即時更新進度條和文字顯示區。
10. **任務完成**：所有音訊塊處理完畢後，Worker 將最終結果（包含時間戳記的完整文字列表）儲存起來，並發送完成信號。前端提示使用者任務已完成，並顯示下載按鈕。
11. **下載檔案**：使用者點擊下載按鈕，前端向後端發送請求，後端根據使用者需要的格式（純文字、SRT 等）生成檔案並回傳。

---

## 3. 後端開發 (FastAPI)

### 3.1. API 端點設計

* `POST /api/v1/transcribe`
    * **功能**: 接收上傳的音訊檔案及設定。
    * **請求 Body (multipart/form-data)**:
        * `file`: 音訊檔案。
        * `model_choice`: 字串 (`"vertex_ai"` 或 `"remote_llm"`)。
        * `start_time` (可選): 字串，格式 `HH:MM:SS`。
        * `end_time` (可選): 字串，格式 `HH:MM:SS`。
    * **成功回應 (202)**: `{ "task_id": "some-unique-task-id" }`

* `WS /ws/v1/status/{task_id}`
    * **功能**: 建立 WebSocket 連線以接收即時進度。
    * **伺服器推播訊息格式 (JSON)**:
        * 處理中: `{ "status": "processing", "progress": 35.5, "partial_text": "這是一段剛辨識出來的文字..." }`
        * 完成: `{ "status": "completed" }`
        * 失敗: `{ "status": "failed", "error": "錯誤訊息" }`

* `GET /api/v1/result/{task_id}`
    * **功能**: 任務完成後，獲取最終結果或下載檔案。
    * **查詢參數**: `format` (`"plain"`, `"timestamped"`, `"srt"`)。
    * **成功回應 (200)**: 回傳對應格式的檔案。

### 3.2. 音訊處理與任務管理

在 Celery 的背景任務中，實作以下邏輯：

1.  **儲存與讀取**：將上傳的檔案暫存到伺服器磁碟。
2.  **時間軸裁切**：如果使用者提供了 `start_time` 和 `end_time`，使用 `pydub` 函式庫對音訊進行裁切。
3.  **音訊分塊**：將裁切後的音訊分割成多個小於 API 限制（例如 Vertex AI 的 1 分鐘限制或遠端模型的記憶體限制）的臨時檔案。
4.  **迭代呼叫**：迴圈處理每一個音訊塊，並根據選擇的模型呼叫對應的函式。
5.  **結果彙整**：將每個塊回傳的文字與時間戳記資料存入一個列表中。
6.  **進度更新**：每處理完一個塊，就透過 WebSocket 發送一次進度更新。

---

## 4. 遠端模型伺服器開發 (RTX 5060 Ti)

這是一個獨立的 FastAPI 應用程式，部署在擁有 RTX 5060 Ti 的遠端機器上。

### 4.1. 選擇開源模型

* **推薦模型**: **`insanely-fast-whisper`**
* **原因**: 這是 OpenAI Whisper 的一個高度優化版本，利用 `optimum` 和 `flash-attention-2` 等技術，在保持高準確度的同時，大幅提升了在 NVIDIA GPU 上的推論速度，非常適合 16GB VRAM 的顯卡。

### 4.2. 伺服器腳本 (`remote_inference_server.py`)

```python
# remote_inference_server.py
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
import torch
from transformers import pipeline
import tempfile
import os

# --- 初始化 ---
app = FastAPI()

# 檢查是否有可用的 GPU
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
# 根據 VRAM 調整模型大小，"large-v3" 可能需要更多記憶體
# "distil-large-v2" 是一個速度與品質兼具的好選擇
MODEL_NAME = "openai/whisper-large-v3" 
BATCH_SIZE = 8 # 根據 VRAM 調整

# 載入模型 Pipeline
pipe = pipeline(
    "automatic-speech-recognition",
    model=MODEL_NAME,
    torch_dtype=torch.float16,
    device=DEVICE,
)

print(f"模型 {MODEL_NAME} 已成功載入至 {DEVICE}")

class TranscriptionResponse(BaseModel):
    text: str
    language: str

@app.post("/transcribe/", response_model=TranscriptionResponse)
async def transcribe_audio(file: UploadFile = File(...)):
    """接收音訊檔案並回傳辨識結果"""
    try:
        # FastAPI 的 UploadFile 是一個類檔案物件，我們需要將其內容寫入臨時檔案
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # 使用 pipeline 進行辨識
        outputs = pipe(
            tmp_path,
            chunk_length_s=30,
            batch_size=BATCH_SIZE,
            return_timestamps=False, # 在遠端只回傳純文字，時間戳由主伺服器計算
        )
        
        # 清理臨時檔案
        os.remove(tmp_path)
        
        return {
            "text": outputs["text"],
            "language": outputs.get("language", "unknown")
        }
    except Exception as e:
        return {"text": f"Error: {str(e)}", "language": "error"}
```

---

## 5. 前端開發 (React/Vue)

### 5.1. 核心邏輯

1.  **檔案上傳元件**：使用 `Ant Design` 或 `Material-UI` 的上傳元件，限制檔案類型和大小。
2.  **狀態管理**：使用 `useState` 或狀態管理庫 (如 Redux, Zustand) 來追蹤以下狀態：
    * `file`: 已選擇的檔案。
    * `modelChoice`: 使用者選擇的模型。
    * `taskStatus`: (`idle`, `uploading`, `processing`, `completed`, `failed`)。
    * `progress`: 數值，0-100。
    * `transcribedText`: 字串，用於即時顯示結果。
    * `taskId`: 從後端獲取的任務 ID。
3.  **發起轉錄**：點擊「開始」按鈕後，將檔案和選項 `POST` 到後端的 `/api/v1/transcribe`。
4.  **建立 WebSocket 連線**：成功獲取 `taskId` 後，立即初始化 WebSocket 連線到 `ws/v1/status/{taskId}`。
5.  **監聽訊息**：在 WebSocket 的 `onmessage` 事件處理函式中，解析後端傳來的 JSON 資料，並更新 `progress` 和 `transcribedText` 狀態，觸發 UI 重新渲染。
6.  **結果下載**：當 `taskStatus` 變為 `completed` 時，顯示下載按鈕。點擊按鈕時，向 `GET /api/v1/result/{taskId}?format=...` 發起請求來下載不同格式的檔案。

---

## 6. 輸出格式生成

在後端主伺服器中，當任務完成後，根據儲存的帶有時間戳記的文字列表，生成不同格式的檔案。

### 6.1. SRT 格式生成範例

```python
def to_srt_time_format(seconds):
    """將秒數轉換為 SRT 的時間格式 HH:MM:SS,ms"""
    millisec = int((seconds - int(seconds)) * 1000)
    minutes, seconds = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours:02}:{minutes:02}:{seconds:02},{millisec:03}"

def generate_srt(transcription_data):
    """
    從 transcription_data 列表生成 SRT 格式內容。
    transcription_data 格式: [{"start": 0.5, "end": 4.2, "text": "你好"}, ...]
    """
    srt_content = []
    for i, segment in enumerate(transcription_data, 1):
        start_time = to_srt_time_format(segment['start'])
        end_time = to_srt_time_format(segment['end'])
        text = segment['text']
        
        srt_content.append(str(i))
        srt_content.append(f"{start_time} --> {end_time}")
        srt_content.append(text)
        srt_content.append("") # 空行分隔
        
    return "\n".join(srt_content)
```

這份文件為您提供了一個清晰的藍圖。下一步，您可以從**搭建後端 FastAPI 專案骨架**和**編寫遠端模型伺服器腳本**開始。祝您開發順利！
